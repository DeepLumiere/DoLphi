{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.948539700Z",
     "start_time": "2025-09-13T09:08:11.376462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%env HUGGINGFACE_HUB_CACHE= models\n",
    "%env TORCH_HOME=models"
   ],
   "id": "58f927978943ebe3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HUGGINGFACE_HUB_CACHE=models\n",
      "env: TORCH_HOME=models\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.949539900Z",
     "start_time": "2025-09-13T09:18:51.198749Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from moviepy import VideoFileClip\n",
    "import os\n",
    "import timm\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration, M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import soundfile as sf\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from PIL import Image\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, precision_score, recall_score, f1_score, normalized_mutual_info_score, \\\n",
    "    adjusted_rand_score\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import yt_dlp\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.950540600Z",
     "start_time": "2025-09-13T09:18:52.596753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ],
   "id": "5a5ca14a65e4d913",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.961539300Z",
     "start_time": "2025-09-13T09:18:52.809511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_audio(url, audio_format='mp3'):\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': audio_format,\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': 'extracted_audi/%(title)s.%(ext)s',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([url])\n",
    "        print(f\"Successfully downloaded and saved audio as {audio_format}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ],
   "id": "fcc8e03c3f3e0410",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.962540100Z",
     "start_time": "2025-09-13T09:18:53.297105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_audio(input_file, output_folder=\"extracted_audio\"):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    base_name = os.path.basename(input_file)\n",
    "    file_name, file_ext = os.path.splitext(base_name)\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, f\"{file_name}.mp3\")\n",
    "\n",
    "    if file_ext.lower() == \".mp4\":\n",
    "        print(f\"Detected MP4 file. Extracting audio from '{input_file}'...\")\n",
    "        try:\n",
    "            video_clip = VideoFileClip(input_file)\n",
    "            audio_clip = video_clip.audio\n",
    "            audio_clip.write_audiofile(output_file_path)\n",
    "            audio_clip.close()\n",
    "            video_clip.close()\n",
    "            print(f\"Audio extracted successfully and saved to '{output_file_path}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during MP4 processing: {e}\")\n",
    "\n",
    "    elif file_ext.lower() == \".mp3\":\n",
    "        print(f\"Detected MP3 file. Copying '{input_file}'...\")\n",
    "        try:\n",
    "            with open(input_file, 'rb') as f_in, open(output_file_path, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "            return f_out\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during MP3 processing: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_ext}. Please provide an MP4 or MP3 file.\")\n",
    "        return None"
   ],
   "id": "5d40abacee3d15a9",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.963540300Z",
     "start_time": "2025-09-13T09:18:53.677021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", torch_dtype=torch_dtype).to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    generate_kwargs={\"task\": \"transcribe\"}\n",
    ")\n"
   ],
   "id": "c3ff29f8616f028",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.963540300Z",
     "start_time": "2025-09-13T09:19:02.601041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def audio_to_text(audio):\n",
    "    data, samplerate = sf.read(audio)\n",
    "\n",
    "    if len(data.shape) > 1:\n",
    "        mono_data = np.mean(data, axis=1)\n",
    "    else:\n",
    "        mono_data = data\n",
    "\n",
    "    result = pipe({\"array\": mono_data, \"sampling_rate\": samplerate}, return_timestamps=True)\n",
    "\n",
    "    text = result[\"text\"]\n",
    "    detected_lang = result.get(\"language\", None)\n",
    "\n",
    "    return text, detected_lang\n"
   ],
   "id": "2b7135168a71882a",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.964540300Z",
     "start_time": "2025-09-13T09:19:02.617796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trans_model_name = \"facebook/m2m100_418M\"\n",
    "trans_tokenizer = M2M100Tokenizer.from_pretrained(trans_model_name)\n",
    "trans_model = M2M100ForConditionalGeneration.from_pretrained(trans_model_name).to(device)"
   ],
   "id": "2c55f9edc72dd5b9",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.965539700Z",
     "start_time": "2025-09-13T09:19:10.832651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate_to_english(text, source_lang):\n",
    "    if source_lang is None:\n",
    "        return text\n",
    "\n",
    "    trans_tokenizer.src_lang = source_lang\n",
    "\n",
    "    encoded = trans_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    generated_tokens = trans_model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=trans_tokenizer.get_lang_id(\"en\")\n",
    "    )\n",
    "\n",
    "    translation = trans_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    return translation"
   ],
   "id": "798a1824b842f832",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.965539700Z",
     "start_time": "2025-09-13T09:19:10.856402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')"
   ],
   "id": "b28995e4f006ded6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dudec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.966538900Z",
     "start_time": "2025-09-13T09:19:10.929741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lemmatize_text(text):\n",
    "    words = text.lower().split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return \" \".join(lemmatized_words)"
   ],
   "id": "5e8d6f1694c61b45",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.968538500Z",
     "start_time": "2025-09-13T09:19:10.956917Z"
    }
   },
   "cell_type": "code",
   "source": "model = SentenceTransformer('all-MiniLM-L6-v2')",
   "id": "8a28446dbde92269",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.969540700Z",
     "start_time": "2025-09-13T09:19:18.391891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_to_vector(text):\n",
    "    text_sentences = [s.strip() for s in re.split(r'[.?!]\\s+', text) if s.strip()]\n",
    "    text_embeddings = model.encode(text_sentences)\n",
    "    return text_embeddings"
   ],
   "id": "dd0c378ee2946340",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.970538500Z",
     "start_time": "2025-09-13T09:19:18.406997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vit_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "# vit_model.eval()\n",
    "# vit_model.reset_classifier(0)\n",
    "#\n",
    "# transform = T.Compose([\n",
    "#     T.Resize((224, 224)),\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "# ])"
   ],
   "id": "934aa04097759886",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.971538100Z",
     "start_time": "2025-09-13T09:19:18.471165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def image_to_vector(image_path):\n",
    "#     img = Image.open(image_path).convert(\"RGB\")\n",
    "#     x = transform(img).unsqueeze(0)\n",
    "#\n",
    "#     with torch.no_grad():\n",
    "#         patches = vit_model.patch_embed(x)\n",
    "#\n",
    "#     patch_vectors = patches.squeeze(0).numpy()\n",
    "#\n",
    "#     norm = np.linalg.norm(patch_vectors, axis=1, keepdims=True)\n",
    "#     normalized_vectors = patch_vectors / norm\n",
    "#\n",
    "#     return normalized_vectors"
   ],
   "id": "db8b0b6d530fffc",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.971538100Z",
     "start_time": "2025-09-13T09:19:18.494090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimal_n_clusters(vectarray):\n",
    "    n_samples = len(vectarray)\n",
    "\n",
    "    if n_samples < 3:\n",
    "        print(\"Not enough samples to determine optimal clusters. Defaulting to 1.\")\n",
    "        return 1\n",
    "\n",
    "    max_clusters = min(n_samples - 1, 30)\n",
    "\n",
    "    if max_clusters < 3:\n",
    "        print(\"Too few samples for meaningful clustering range. Defaulting to 2.\")\n",
    "        return 2\n",
    "\n",
    "    range_n_clusters = list(range(3, max_clusters))\n",
    "\n",
    "    if not range_n_clusters:\n",
    "        return max_clusters\n",
    "\n",
    "    silhouette_scores = []\n",
    "    print(f\"Testing cluster counts from 3 to {max_clusters-1}...\")\n",
    "    for n_clusters in range_n_clusters:\n",
    "        if n_clusters >= n_samples:\n",
    "            break\n",
    "\n",
    "        clustering_model = KMeans(n_clusters=n_clusters, random_state=0, n_init='auto')\n",
    "        cluster_labels = clustering_model.fit_predict(vectarray)\n",
    "\n",
    "        score = silhouette_score(vectarray, cluster_labels)\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "    if not silhouette_scores:\n",
    "        return 2\n",
    "\n",
    "    optimal_n = range_n_clusters[np.argmax(silhouette_scores)]\n",
    "    print(f\"Optimal number of clusters found: {optimal_n}\")\n",
    "    return optimal_n"
   ],
   "id": "219f6a484de6e0fd",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.972539800Z",
     "start_time": "2025-09-13T09:19:18.509016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vector_merger(vectarray):\n",
    "    n = optimal_n_clusters(vectarray)\n",
    "    final_clustering_model = KMeans(n_clusters=n, random_state=0, n_init=10)\n",
    "    merger_model = final_clustering_model.fit_predict(vectarray)\n",
    "\n",
    "    merged_embeddings = []\n",
    "    for i in range(n):\n",
    "        cluster_indices = np.argwhere(merger_model == i).flatten()\n",
    "        if len(cluster_indices) > 0:\n",
    "            cluster_embeddings = vectarray[cluster_indices]\n",
    "            cluster_centroid = np.mean(cluster_embeddings, axis=0)\n",
    "\n",
    "            norm = np.linalg.norm(cluster_centroid)\n",
    "            if norm > 0:\n",
    "                cluster_centroid = cluster_centroid / norm\n",
    "\n",
    "            merged_embeddings.append(cluster_centroid)\n",
    "\n",
    "    return np.array(merged_embeddings), merger_model"
   ],
   "id": "9a18d8e0825a1903",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.973537600Z",
     "start_time": "2025-09-13T09:19:18.528413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def faiss_scoring(base_vectarray, test_vectarray):\n",
    "    test_vectarray = np.array(test_vectarray)\n",
    "    base_vectarray = np.array(base_vectarray)\n",
    "\n",
    "    faiss_index = faiss.IndexFlatIP(test_vectarray.shape[1])\n",
    "    faiss_index.add(test_vectarray)\n",
    "\n",
    "    distances, _ = faiss_index.search(base_vectarray, k=1)\n",
    "\n",
    "    similarity_scores = distances.flatten()\n",
    "\n",
    "    return similarity_scores"
   ],
   "id": "6b5f523cea92c6f6",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.973537600Z",
     "start_time": "2025-09-13T09:19:18.546890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0.0"
   ],
   "id": "fa7364bc3220eaac",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.974539Z",
     "start_time": "2025-09-13T09:19:18.560449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def coverage_score(base_vecs, test_vecs, threshold=0.7):\n",
    "    sim_matrix = cosine_similarity(base_vecs, test_vecs)\n",
    "    covered = np.sum(np.max(sim_matrix, axis=1) >= threshold)\n",
    "    return covered / len(base_vecs)"
   ],
   "id": "f554d3d32fb45ad1",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.974539Z",
     "start_time": "2025-09-13T09:19:18.577828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_clustered_vectors(base_vecs, test_vecs, base_text, test_text, coverage_threshold=[0.7]):\n",
    "    results = {}\n",
    "\n",
    "    # --- Vector-based similarity checks ---\n",
    "    sim_matrix = cosine_similarity(base_vecs, test_vecs)\n",
    "    best_matches_base_to_test = np.max(sim_matrix, axis=1)\n",
    "    results[\"Asymmetric Mean Similarity (Base to Test)\"] = float(np.mean(best_matches_base_to_test))\n",
    "    best_matches_test_to_base = np.max(sim_matrix, axis=0)\n",
    "    results[\"Asymmetric Mean Similarity (Test to Base)\"] = float(np.mean(best_matches_test_to_base))\n",
    "\n",
    "    for i in coverage_threshold:\n",
    "        results[f\"Coverage (Base in Test) @{i}\"] = np.sum(best_matches_base_to_test >= i) / len(base_vecs)\n",
    "\n",
    "    base_vecs_32 = np.ascontiguousarray(base_vecs.astype('float32'))\n",
    "    test_vecs_32 = np.ascontiguousarray(test_vecs.astype('float32'))\n",
    "\n",
    "    index = faiss.IndexFlatIP(test_vecs_32.shape[1])\n",
    "    index.add(test_vecs_32)\n",
    "    faiss_scores, _ = index.search(base_vecs_32, k=1)\n",
    "\n",
    "    results[\"FAISS Similarity (Max of Best Matches)\"] = float(np.max(faiss_scores))\n",
    "    results[\"FAISS Similarity (Mean of Best Matches)\"] = float(np.mean(faiss_scores))\n",
    "\n",
    "    # --- Text-based similarity checks ---\n",
    "    # BLEU score\n",
    "    try:\n",
    "        # Sentence BLEU needs a list of tokenized words\n",
    "        base_tokens = base_text.lower().split()\n",
    "        test_tokens = test_text.lower().split()\n",
    "\n",
    "        # Use a smoothing function for better results with short sentences\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        bleu_score = sentence_bleu([base_tokens], test_tokens, smoothing_function=smoothie)\n",
    "        results[\"BLEU Score\"] = bleu_score\n",
    "    except Exception as e:\n",
    "        results[\"BLEU Score\"] = f\"Error: {e}\"\n",
    "\n",
    "    try:\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(base_text, test_text)\n",
    "        results[\"ROUGE-1 Score\"] = scores['rouge1'].fmeasure\n",
    "        results[\"ROUGE-L Score\"] = scores['rougeL'].fmeasure\n",
    "    except Exception as e:\n",
    "        results[\"ROUGE Scores\"] = f\"Error: {e}\"\n",
    "\n",
    "    try:\n",
    "        gts = {0: [base_text]}\n",
    "        res = {0: [test_text]}\n",
    "        cider_scorer = Cider()\n",
    "        (score, _) = cider_scorer.compute_score(gts, res)\n",
    "        results[\"CIDEr Score\"] = float(score)\n",
    "    except Exception as e:\n",
    "        results[\"CIDEr Score\"] = f\"Error: {e}\"\n",
    "\n",
    "    return results"
   ],
   "id": "e1bc422225da30",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.975539300Z",
     "start_time": "2025-09-13T09:19:18.596580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_and_evaluate(data_type, base_path, test_path, coverage_thresh):\n",
    "    if data_type == 'text':\n",
    "        with open(base_path, 'r') as f:\n",
    "            base_content = f.read()\n",
    "        base_vectors = text_to_vector(lemmatize_text(base_content))\n",
    "\n",
    "        with open(test_path, 'r') as f:\n",
    "            test_content = f.read()\n",
    "        test_vectors = text_to_vector(lemmatize_text(test_content))\n",
    "\n",
    "        base_translated = base_content\n",
    "        test_translated = test_content\n",
    "\n",
    "    elif data_type == 'audio':\n",
    "        # --- Base file ---\n",
    "        base_text, base_lang = audio_to_text(base_path)\n",
    "        base_translated = translate_to_english(base_text, base_lang)\n",
    "        base_vectors = text_to_vector(lemmatize_text(base_translated))\n",
    "\n",
    "        # --- Test file ---\n",
    "        test_text, test_lang = audio_to_text(test_path)\n",
    "        test_translated = translate_to_english(test_text, test_lang)\n",
    "        test_vectors = text_to_vector(lemmatize_text(test_translated))\n",
    "\n",
    "        print(f\"[Base Audio] Detected {base_lang}, transcribed: {base_text}\")\n",
    "        print(f\"[Base Audio] English translation: {base_translated}\")\n",
    "        print(f\"[Test Audio] Detected {test_lang}, transcribed: {test_text}\")\n",
    "        print(f\"[Test Audio] English translation: {test_translated}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Invalid data_type. Please choose 'text', 'image', or 'audio'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nBase data has {len(base_vectors)} initial vectors.\")\n",
    "    print(f\"Test data has {len(test_vectors)} initial vectors.\")\n",
    "\n",
    "    print(\"\\nFinding optimal clusters and merging base vectors...\")\n",
    "    base_merged_vectors, _ = vector_merger(base_vectors)\n",
    "    print(f\"Created {len(base_merged_vectors)} base cluster centroids.\")\n",
    "\n",
    "    print(\"\\nFinding optimal clusters and merging test vectors...\")\n",
    "    test_merged_vectors, _ = vector_merger(test_vectors)\n",
    "    print(f\"Created {len(test_merged_vectors)} test cluster centroids.\")\n",
    "\n",
    "    # Pass the translated text to the evaluation function\n",
    "    evaluation_results = evaluate_clustered_vectors(\n",
    "        base_merged_vectors,\n",
    "        test_merged_vectors,\n",
    "        base_translated,\n",
    "        test_translated,\n",
    "        coverage_threshold=coverage_thresh\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Evaluation Summary ---\")\n",
    "    print(f\"Data Type Compared: {data_type}\")\n",
    "    print(f\"Base Path: {base_path} ({len(base_merged_vectors)} centroids)\")\n",
    "    print(f\"Test Path: {test_path} ({len(test_merged_vectors)} centroids)\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Asymmetric Mean Similarity (Base to Test): {evaluation_results['Asymmetric Mean Similarity (Base to Test)']:.4f}\")\n",
    "    print(f\"Asymmetric Mean Similarity (Test to Base): {evaluation_results['Asymmetric Mean Similarity (Test to Base)']:.4f}\")\n",
    "\n",
    "    for i in coverage_thresh:\n",
    "        print(f\"Coverage (Base in Test) @{i}:              {evaluation_results[f'Coverage (Base in Test) @{i} ']:.4f}\")\n",
    "\n",
    "    print(f\"FAISS Similarity (Max of Best Matches):    {evaluation_results['FAISS Similarity (Max of Best Matches)']:.4f}\")\n",
    "    print(f\"FAISS Similarity (Mean of Best Matches):   {evaluation_results['FAISS Similarity (Mean of Best Matches)']:.4f}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"BLEU Score:                                {evaluation_results['BLEU Score']:.4f}\")\n",
    "    print(f\"ROUGE-1 Score:                             {evaluation_results.get('ROUGE-1 Score', 'N/A'):.4f}\")\n",
    "    print(f\"CIDEr Score:                               {evaluation_results.get('CIDEr Score', 'N/A'):.4f}\")\n",
    "    print(\"--------------------------------------------------\\n\")"
   ],
   "id": "bfe989e8f13c3f05",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.945542300Z",
     "start_time": "2025-09-13T09:19:18.615055Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 43,
   "source": [
    "DATA_TYPE_CHOICE = ('text')\n",
    "\n",
    "if DATA_TYPE_CHOICE == 'text':\n",
    "    BASE_FILE_PATH = 'data/base_text.txt'\n",
    "    TEST_FILE_PATH = 'data/test_text.txt'\n",
    "\n",
    "elif DATA_TYPE_CHOICE == 'image':\n",
    "    BASE_FILE_PATH = 'data/base_image.png'\n",
    "    TEST_FILE_PATH = 'data/test_image.png'\n",
    "\n",
    "elif DATA_TYPE_CHOICE == 'audio':\n",
    "    BASE_FILE_PATH = 'extracted_audio/AI and future of work.mp3'\n",
    "    # TEST_FILE_PATH = 'extracted_audio/lR-ip0EZQXS_uczWPpahSQ_b9e17546063c4d59822e7488419afbf1_200826.005_MP4_720.mp3'\n",
    "    TEST_FILE_PATH = 'extracted_audio/AI and future of work.mp3'\n",
    "\n",
    "COVERAGE_THRESHOLD = [0.7, 0.75, 0.8, 0.85, 0.9, 0.95]"
   ],
   "id": "8f4706a52860e71c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T09:28:25.758132Z",
     "start_time": "2025-09-13T09:19:18.631458Z"
    }
   },
   "cell_type": "code",
   "source": "process_and_evaluate(DATA_TYPE_CHOICE, BASE_FILE_PATH, TEST_FILE_PATH, COVERAGE_THRESHOLD)",
   "id": "92948cf006ec9722",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[44]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mprocess_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA_TYPE_CHOICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBASE_FILE_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTEST_FILE_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCOVERAGE_THRESHOLD\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[42]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mprocess_and_evaluate\u001B[39m\u001B[34m(data_type, base_path, test_path, coverage_thresh)\u001B[39m\n\u001B[32m     12\u001B[39m     test_translated = test_content\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m data_type == \u001B[33m'\u001B[39m\u001B[33maudio\u001B[39m\u001B[33m'\u001B[39m:\n\u001B[32m     15\u001B[39m     \u001B[38;5;66;03m# --- Base file ---\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     base_text, base_lang = \u001B[43maudio_to_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m     base_translated = translate_to_english(base_text, base_lang)\n\u001B[32m     18\u001B[39m     base_vectors = text_to_vector(lemmatize_text(base_translated))\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 9\u001B[39m, in \u001B[36maudio_to_text\u001B[39m\u001B[34m(audio)\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m      7\u001B[39m     mono_data = data\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m result = \u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43marray\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmono_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msampling_rate\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplerate\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_timestamps\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m text = result[\u001B[33m\"\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     12\u001B[39m detected_lang = result.get(\u001B[33m\"\u001B[39m\u001B[33mlanguage\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:280\u001B[39m, in \u001B[36mAutomaticSpeechRecognitionPipeline.__call__\u001B[39m\u001B[34m(self, inputs, **kwargs)\u001B[39m\n\u001B[32m    223\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs: Union[np.ndarray, \u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mdict\u001B[39m], **kwargs: Any) -> \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    224\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    225\u001B[39m \u001B[33;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001B[39;00m\n\u001B[32m    226\u001B[39m \u001B[33;03m    documentation for more information.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    278\u001B[39m \u001B[33;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001B[39;00m\n\u001B[32m    279\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m280\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\base.py:1450\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1448\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001B[32m   1449\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ChunkPipeline):\n\u001B[32m-> \u001B[39m\u001B[32m1450\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m   1451\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m   1452\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_iterator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1453\u001B[39m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\n\u001B[32m   1454\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1455\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1456\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1457\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1458\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001B[39m, in \u001B[36mPipelineIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    121\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_item()\n\u001B[32m    123\u001B[39m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m item = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    125\u001B[39m processed = \u001B[38;5;28mself\u001B[39m.infer(item, **\u001B[38;5;28mself\u001B[39m.params)\n\u001B[32m    126\u001B[39m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001B[39m, in \u001B[36mPipelinePackIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    266\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m accumulator\n\u001B[32m    268\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_last:\n\u001B[32m--> \u001B[39m\u001B[32m269\u001B[39m     processed = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    270\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    271\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed, torch.Tensor):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\base.py:1365\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1363\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1364\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1366\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1367\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:542\u001B[39m, in \u001B[36mAutomaticSpeechRecognitionPipeline._forward\u001B[39m\u001B[34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001B[39m\n\u001B[32m    536\u001B[39m main_input_name = \u001B[38;5;28mself\u001B[39m.model.main_input_name \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.model, \u001B[33m\"\u001B[39m\u001B[33mmain_input_name\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33minputs\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    537\u001B[39m generate_kwargs = {\n\u001B[32m    538\u001B[39m     main_input_name: inputs,\n\u001B[32m    539\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mattention_mask\u001B[39m\u001B[33m\"\u001B[39m: attention_mask,\n\u001B[32m    540\u001B[39m     **generate_kwargs,\n\u001B[32m    541\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m542\u001B[39m tokens = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    544\u001B[39m \u001B[38;5;66;03m# whisper longform generation stores timestamps in \"segments\"\u001B[39;00m\n\u001B[32m    545\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_timestamps == \u001B[33m\"\u001B[39m\u001B[33mword\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.type == \u001B[33m\"\u001B[39m\u001B[33mseq2seq_whisper\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:866\u001B[39m, in \u001B[36mWhisperGenerationMixin.generate\u001B[39m\u001B[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, monitor_progress, **kwargs)\u001B[39m\n\u001B[32m    857\u001B[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001B[32m1\u001B[39m])\n\u001B[32m    859\u001B[39m \u001B[38;5;66;03m# 6.6 Run generate with fallback\u001B[39;00m\n\u001B[32m    860\u001B[39m (\n\u001B[32m    861\u001B[39m     seek_sequences,\n\u001B[32m    862\u001B[39m     seek_outputs,\n\u001B[32m    863\u001B[39m     should_skip,\n\u001B[32m    864\u001B[39m     do_condition_on_prev_tokens,\n\u001B[32m    865\u001B[39m     model_output_type,\n\u001B[32m--> \u001B[39m\u001B[32m866\u001B[39m ) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_with_fallback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    867\u001B[39m \u001B[43m    \u001B[49m\u001B[43msegment_input\u001B[49m\u001B[43m=\u001B[49m\u001B[43msegment_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    868\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    869\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcur_bsz\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcur_bsz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    870\u001B[39m \u001B[43m    \u001B[49m\u001B[43mseek\u001B[49m\u001B[43m=\u001B[49m\u001B[43mseek\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    871\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_idx_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_idx_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    872\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtemperatures\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtemperatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    873\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprefix_allowed_tokens_fn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefix_allowed_tokens_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m    \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_token_timestamps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_timestamps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdo_condition_on_prev_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdo_condition_on_prev_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    880\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_shortform\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_shortform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    884\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    886\u001B[39m \u001B[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001B[39;00m\n\u001B[32m    887\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, seek_sequence \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(seek_sequences):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:1038\u001B[39m, in \u001B[36mWhisperGenerationMixin.generate_with_fallback\u001B[39m\u001B[34m(self, segment_input, decoder_input_ids, cur_bsz, seek, batch_idx_map, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001B[39m\n\u001B[32m   1033\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m generate_kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mencoder_outputs\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1034\u001B[39m         generate_kwargs[\u001B[33m\"\u001B[39m\u001B[33mencoder_outputs\u001B[39m\u001B[33m\"\u001B[39m] = F.pad(\n\u001B[32m   1035\u001B[39m             generate_kwargs[\u001B[33m\"\u001B[39m\u001B[33mencoder_outputs\u001B[39m\u001B[33m\"\u001B[39m], (\u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m, batch_size - cur_bsz), value=\u001B[32m0\u001B[39m\n\u001B[32m   1036\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1038\u001B[39m seek_outputs = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1039\u001B[39m \u001B[43m    \u001B[49m\u001B[43msegment_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1040\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1041\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1042\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1043\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprefix_allowed_tokens_fn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefix_allowed_tokens_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1044\u001B[39m \u001B[43m    \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1045\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1046\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1047\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1048\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1050\u001B[39m model_output_type = \u001B[38;5;28mtype\u001B[39m(seek_outputs)\n\u001B[32m   1052\u001B[39m \u001B[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\utils.py:2636\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[39m\n\u001B[32m   2629\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2630\u001B[39m         input_ids=input_ids,\n\u001B[32m   2631\u001B[39m         expand_size=generation_config.num_beams,\n\u001B[32m   2632\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2633\u001B[39m         **model_kwargs,\n\u001B[32m   2634\u001B[39m     )\n\u001B[32m   2635\u001B[39m     \u001B[38;5;66;03m# 12. run beam sample\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2636\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2637\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2638\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2639\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2640\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2641\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2642\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2643\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2645\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001B[32m   2646\u001B[39m     logger.warning_once(\n\u001B[32m   2647\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2648\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2649\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\utils.py:4098\u001B[39m, in \u001B[36mGenerationMixin._beam_search\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001B[39m\n\u001B[32m   4095\u001B[39m \u001B[38;5;66;03m# b. Compute log probs -- get log probabilities from logits, process logits with processors (*e.g.*\u001B[39;00m\n\u001B[32m   4096\u001B[39m \u001B[38;5;66;03m# `temperature`, ...), and add new logprobs to existing running logprobs scores.\u001B[39;00m\n\u001B[32m   4097\u001B[39m log_probs = nn.functional.log_softmax(logits, dim=-\u001B[32m1\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m4098\u001B[39m log_probs = \u001B[43mlogits_processor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflat_running_sequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_probs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4100\u001B[39m \u001B[38;5;66;03m# Store logits, attentions and hidden_states when required\u001B[39;00m\n\u001B[32m   4101\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_dict_in_generate:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\logits_process.py:93\u001B[39m, in \u001B[36mLogitsProcessorList.__call__\u001B[39m\u001B[34m(self, input_ids, scores, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         scores = processor(input_ids, scores, **kwargs)\n\u001B[32m     92\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m         scores = \u001B[43mprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscores\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m scores\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\logits_process.py:2043\u001B[39m, in \u001B[36mWhisperTimeStampLogitsProcessor.__call__\u001B[39m\u001B[34m(self, input_ids, scores)\u001B[39m\n\u001B[32m   2040\u001B[39m         scores_processed[k, : \u001B[38;5;28mself\u001B[39m.eos_token_id] = -\u001B[38;5;28mfloat\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33minf\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   2042\u001B[39m timestamps = sampled_tokens[sampled_tokens.ge(\u001B[38;5;28mself\u001B[39m.timestamp_begin)]\n\u001B[32m-> \u001B[39m\u001B[32m2043\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtimestamps\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnumel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m > \u001B[32m0\u001B[39m:\n\u001B[32m   2044\u001B[39m     \u001B[38;5;66;03m# `timestamps` shouldn't decrease; forbid timestamp tokens smaller than the last\u001B[39;00m\n\u001B[32m   2045\u001B[39m     \u001B[38;5;66;03m# The following lines of code are copied from: https://github.com/openai/whisper/pull/914/files#r1137085090\u001B[39;00m\n\u001B[32m   2046\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m last_was_timestamp \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m penultimate_was_timestamp:\n\u001B[32m   2047\u001B[39m         timestamp_last = timestamps[-\u001B[32m1\u001B[39m]\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c40c7c052833300"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
