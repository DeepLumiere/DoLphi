{
 "cells": [
  {
   "cell_type": "code",
   "id": "b2d98f4e79af9ce6",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-27T11:01:17.026025Z"
    }
   },
   "source": [
    "\n",
    "!pip install torch transformers soundfile moviepy numpy pandas nltk faiss-cpu"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "834f6e3d243ddd2b",
   "metadata": {},
   "source": [
    "%env HUGGINGFACE_HUB_CACHE= models"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from moviepy import VideoFileClip, AudioFileClip\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline, WhisperProcessor, \\\n",
    "    WhisperForConditionalGeneration\n",
    "# from nemo.collections.asr.models import ClusteringDiarizer\n",
    "import soundfile as sf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ],
   "id": "71e70a943c336cb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98c54699f2de8a13",
   "metadata": {},
   "source": [
    "def extract_audio(input_file, output_folder=\"extracted_audio\"):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    base_name = os.path.basename(input_file)\n",
    "    file_name, file_ext = os.path.splitext(base_name)\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, f\"{file_name}.mp3\")\n",
    "\n",
    "    if file_ext.lower() == \".mp4\":\n",
    "        print(f\"Detected MP4 file. Extracting audio from '{input_file}'...\")\n",
    "        try:\n",
    "            video_clip = VideoFileClip(input_file)\n",
    "            audio_clip = video_clip.audio\n",
    "            audio_clip.write_audiofile(output_file_path)\n",
    "            audio_clip.close()\n",
    "            video_clip.close()\n",
    "            print(f\"Audio extracted successfully and saved to '{output_file_path}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during MP4 processing: {e}\")\n",
    "\n",
    "    elif file_ext.lower() == \".mp3\":\n",
    "        print(f\"Detected MP3 file. Copying '{input_file}'...\")\n",
    "        try:\n",
    "            with open(input_file, 'rb') as f_in, open(output_file_path, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "            return f_out\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during MP3 processing: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_ext}. Please provide an MP4 or MP3 file.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", torch_dtype=torch_dtype).to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "\n",
    "    generate_kwargs={\"language\": \"en\", \"task\": \"transcribe\"}\n",
    ")"
   ],
   "id": "c311f99137b6b767",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab29c8c2bc82cd70",
   "metadata": {},
   "source": [
    "def audio_to_text(audio):\n",
    "    data, samplerate = sf.read(audio)\n",
    "\n",
    "    if len(data.shape) > 1:\n",
    "        mono_data = np.mean(data, axis=1)\n",
    "    else:\n",
    "        mono_data = data\n",
    "\n",
    "    audtext = pipe({\"array\": mono_data, \"sampling_rate\": samplerate}, return_timestamps=True)\n",
    "\n",
    "    return audtext"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73453bf4c2f4aecd",
   "metadata": {},
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def lemmatize_text(text):\n",
    "    words = text.lower().split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return \" \".join(lemmatized_words)"
   ],
   "id": "d6171eb9f626eed8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model = SentenceTransformer('all-MiniLM-L6-v2')",
   "id": "59498570680c1d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def text_to_vector(text):\n",
    "    text_sentences = [s.strip() for s in re.split(r'[.?!]\\s+', text) if s.strip()]\n",
    "    text_embeddings = model.encode(text_sentences)\n",
    "    return text_embeddings"
   ],
   "id": "bbeae4ad2f4aedfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def optimal_n_clusters(vectarray):\n",
    "    range_n_clusters = list(range(5, 15))\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for n_clusters in range_n_clusters:\n",
    "        clustering_model = KMeans(n_clusters=n_clusters, random_state=0, n_init=15)\n",
    "        cluster_labels = clustering_model.fit_predict(vectarray)\n",
    "\n",
    "        score = silhouette_score(vectarray, cluster_labels)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"Number of clusters: {n_clusters}, Silhouette Score: {score:.4f}\")\n",
    "\n",
    "    optimal_n_clusters = range_n_clusters[np.argmax(silhouette_scores)]\n",
    "\n",
    "    return optimal_n_clusters"
   ],
   "id": "c357f5f5e45152cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sentence_merger(vectarray, text):\n",
    "    n = optimal_n_clusters(vectarray)\n",
    "    final_clustering_model = KMeans(n_clusters=n, random_state=0, n_init=10)\n",
    "    merger_model = final_clustering_model.fit_predict(vectarray)\n",
    "    merged_sentences = []\n",
    "    merged_embeddings = []\n",
    "    for i in range(n):\n",
    "        cluster_indices = np.argwhere(merger_model == i).flatten()\n",
    "\n",
    "        if len(cluster_indices) > 0:\n",
    "            cluster_sentences = [text[j] for j in cluster_indices]\n",
    "            cluster_embeddings = vectarray[cluster_indices]\n",
    "\n",
    "            cluster_centroid = np.mean(cluster_embeddings, axis=0)\n",
    "            distances = np.linalg.norm(cluster_embeddings - cluster_centroid, axis=1)\n",
    "            closest_sentence_idx = np.argmin(distances)\n",
    "\n",
    "            representative_sentence = cluster_sentences[closest_sentence_idx]\n",
    "            merged_sentences.append(representative_sentence)\n",
    "            merged_embeddings.append(cluster_centroid)\n",
    "\n",
    "    return merged_embeddings, merged_sentences"
   ],
   "id": "a1e0f1ab5aec43c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def faiss_scoring(base_vectarray, test_vectarray):\n",
    "    vectarray = np.array(test_vectarray)\n",
    "    faiss_index = faiss.IndexFlatIP(vectarray.shape[1])\n",
    "    faiss_index.add(test_vectarray)\n",
    "\n",
    "    distances, _ = faiss_index.search(base_vectarray, k=1)\n",
    "\n",
    "    similarity_scores = distances.flatten()\n",
    "\n",
    "    return similarity_scores"
   ],
   "id": "c10e2732e4f3c252"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_audio = extract_audio(r\"C:\\Users\\dudec\\OneDrive\\Studies\\Coursera\\Google_AI_Essentials\\AI and future of work.mp4\")\n",
    "\n",
    "test_audio = extract_audio(r\"C:\\Users\\dudec\\OneDrive\\Studies\\Coursera\\Google_AI_Essentials\\AI and future of work.mp4\")"
   ],
   "id": "97da1594a96876f7"
  },
  {
   "cell_type": "code",
   "id": "e4edd538f2820d96",
   "metadata": {},
   "source": [
    "base_text = audio_to_text(base_audio)\n",
    "test_text = audio_to_text(test_audio)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4057b5f79a3ecc97",
   "metadata": {},
   "source": [
    "base_text_lemmat = lemmatize_text(base_text)\n",
    "test_text_lemmat = lemmatize_text(test_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_vectors = text_to_vector(base_text)\n",
    "test_vectors = text_to_vector(test_text)"
   ],
   "id": "c38745f2545ea2e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "merged_base_vectors, merged_base_sentences = sentence_merger(base_vectors, test_vectors)",
   "id": "b89b79997a447e8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "faiss_symscore = faiss_scoring(merged_base_vectors, test_vectors)",
   "id": "c828f9458c57409"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
