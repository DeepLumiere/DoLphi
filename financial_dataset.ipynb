{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-17T12:20:56.368689Z",
     "start_time": "2025-07-17T12:18:39.127870Z"
    }
   },
   "source": [
    "#Importing Packages\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as pre\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:24:32.278807Z",
     "start_time": "2025-07-17T12:24:32.127393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Move to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "59b1997730f08f4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:24:35.702448Z",
     "start_time": "2025-07-17T12:24:35.374325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #Importing data from UNSW_NB15\n",
    "# train_df = pd.read_csv('data/UNSW_NB15_training-set.csv')\n",
    "# test_df = pd.read_csv('data/UNSW_NB15_testing-set.csv')\n",
    "#https://www.kaggle.com/datasets/nikhil1e9/loan-default\n",
    "df = pd.read_csv('data/Loan_default.csv')"
   ],
   "id": "3a8cdff9faac9e5c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:24:36.632831Z",
     "start_time": "2025-07-17T12:24:36.374999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "object_cols = ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply Label Encoding to each object column\n",
    "for col in object_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "train_df, test_df = tts(df)\n"
   ],
   "id": "e4e915c9a4184925",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:24:37.342340Z",
     "start_time": "2025-07-17T12:24:37.324562Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.info()",
   "id": "70b49f8f7d6cd472",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 191510 entries, 64629 to 184431\n",
      "Data columns (total 18 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   LoanID          191510 non-null  object \n",
      " 1   Age             191510 non-null  int64  \n",
      " 2   Income          191510 non-null  int64  \n",
      " 3   LoanAmount      191510 non-null  int64  \n",
      " 4   CreditScore     191510 non-null  int64  \n",
      " 5   MonthsEmployed  191510 non-null  int64  \n",
      " 6   NumCreditLines  191510 non-null  int64  \n",
      " 7   InterestRate    191510 non-null  float64\n",
      " 8   LoanTerm        191510 non-null  int64  \n",
      " 9   DTIRatio        191510 non-null  float64\n",
      " 10  Education       191510 non-null  int64  \n",
      " 11  EmploymentType  191510 non-null  int64  \n",
      " 12  MaritalStatus   191510 non-null  int64  \n",
      " 13  HasMortgage     191510 non-null  int64  \n",
      " 14  HasDependents   191510 non-null  int64  \n",
      " 15  LoanPurpose     191510 non-null  int64  \n",
      " 16  HasCoSigner     191510 non-null  int64  \n",
      " 17  Default         191510 non-null  int64  \n",
      "dtypes: float64(2), int64(15), object(1)\n",
      "memory usage: 27.8+ MB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:24:38.107604Z",
     "start_time": "2025-07-17T12:24:38.094893Z"
    }
   },
   "cell_type": "code",
   "source": "test_df.info()",
   "id": "35c969b3ddc43ad7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 63837 entries, 31069 to 202460\n",
      "Data columns (total 18 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   LoanID          63837 non-null  object \n",
      " 1   Age             63837 non-null  int64  \n",
      " 2   Income          63837 non-null  int64  \n",
      " 3   LoanAmount      63837 non-null  int64  \n",
      " 4   CreditScore     63837 non-null  int64  \n",
      " 5   MonthsEmployed  63837 non-null  int64  \n",
      " 6   NumCreditLines  63837 non-null  int64  \n",
      " 7   InterestRate    63837 non-null  float64\n",
      " 8   LoanTerm        63837 non-null  int64  \n",
      " 9   DTIRatio        63837 non-null  float64\n",
      " 10  Education       63837 non-null  int64  \n",
      " 11  EmploymentType  63837 non-null  int64  \n",
      " 12  MaritalStatus   63837 non-null  int64  \n",
      " 13  HasMortgage     63837 non-null  int64  \n",
      " 14  HasDependents   63837 non-null  int64  \n",
      " 15  LoanPurpose     63837 non-null  int64  \n",
      " 16  HasCoSigner     63837 non-null  int64  \n",
      " 17  Default         63837 non-null  int64  \n",
      "dtypes: float64(2), int64(15), object(1)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:24:38.658026Z",
     "start_time": "2025-07-17T12:24:38.626530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Collecting required features\n",
    "feature_columns = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']# 'attack_cat_encoded'\n",
    "\n",
    "X_train = train_df[feature_columns].values\n",
    "y_train = train_df['Default'].values\n",
    "\n",
    "#\n",
    "# le = pre.LabelEncoder()\n",
    "# train_df['attack_cat_encoded'] = le.fit_transform(train_df['attack_cat'])\n",
    "\n",
    "X_test = test_df[feature_columns].values\n",
    "y_test = test_df['Default'].values"
   ],
   "id": "d879d96c98190a1f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:24:39.373204Z",
     "start_time": "2025-07-17T12:24:39.199647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Converting to tensor\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converting to tensor and moving to the selected device\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)"
   ],
   "id": "81120687472a7ab",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:24:39.826014Z",
     "start_time": "2025-07-17T12:24:39.817538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Data loaded from CSV and prepared for PyTorch.\")\n",
    "print(\"Training data shape:\", X_train_tensor.shape)\n",
    "print(\"Test data shape:\", X_test_tensor.shape)\n",
    "print(f\"Number of unique labels (classes): {len(np.unique(y_train))}\")\n"
   ],
   "id": "6f4f459bdb0bd14b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from CSV and prepared for PyTorch.\n",
      "Training data shape: torch.Size([191510, 1, 16])\n",
      "Test data shape: torch.Size([63837, 1, 16])\n",
      "Number of unique labels (classes): 2\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:24:40.555484Z",
     "start_time": "2025-07-17T12:24:40.547531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net1DCNN(nn.Module):\n",
    "    def __init__(self, input_channels, sequence_length, num_classes):\n",
    "        super(Net1DCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # self.fc1 = nn.Linear(160 * 1, 64)\n",
    "        # self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, input_channels, sequence_length)\n",
    "            dummy_output = self.pool1(self.relu1(self.conv1(dummy_input)))\n",
    "            dummy_output = self.pool2(self.relu2(self.conv2(dummy_output)))\n",
    "            flattened_size = dummy_output.view(dummy_output.size(0), -1).shape[1]\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_size, 128)\n",
    "        self.relu_fc = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "id": "6170ce8e879f03f9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:37:31.394508Z",
     "start_time": "2025-07-17T12:37:24.513604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_channels = X_train_tensor.shape[1]\n",
    "sequence_length = X_train_tensor.shape[2]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "model = Net1DCNN(input_channels=input_channels, sequence_length=sequence_length, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Optimizers\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.85)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training finished.\")"
   ],
   "id": "33716de3afbcb857",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m model.train()\n\u001B[32m     17\u001B[39m running_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    787\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    788\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    791\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n\u001B[32m---> \u001B[39m\u001B[32m55\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001B[39m, in \u001B[36mdefault_collate\u001B[39m\u001B[34m(batch)\u001B[39m\n\u001B[32m    337\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdefault_collate\u001B[39m(batch):\n\u001B[32m    338\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    339\u001B[39m \u001B[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[32m    340\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    396\u001B[39m \u001B[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[32m    397\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m398\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001B[39m, in \u001B[36mcollate\u001B[39m\u001B[34m(batch, collate_fn_map)\u001B[39m\n\u001B[32m    208\u001B[39m transposed = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(*batch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    211\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[32m--> \u001B[39m\u001B[32m212\u001B[39m         \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    213\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[32m    214\u001B[39m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    216\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001B[39m, in \u001B[36mcollate\u001B[39m\u001B[34m(batch, collate_fn_map)\u001B[39m\n\u001B[32m    153\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    154\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[32m--> \u001B[39m\u001B[32m155\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    157\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[32m    158\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mR:\\Apps\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001B[39m, in \u001B[36mcollate_tensor_fn\u001B[39m\u001B[34m(batch, collate_fn_map)\u001B[39m\n\u001B[32m    270\u001B[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001B[32m    271\u001B[39m     out = elem.new(storage).resize_(\u001B[38;5;28mlen\u001B[39m(batch), *\u001B[38;5;28mlist\u001B[39m(elem.size()))\n\u001B[32m--> \u001B[39m\u001B[32m272\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T12:55:11.516587Z",
     "start_time": "2025-07-12T12:55:07.569172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"\\nAccuracy on the test set: {accuracy:.2f}%\")"
   ],
   "id": "40a63e320bb1cf8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on the test set: 84.30%\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_dir = 'models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, '1d_cnn_model.pth')\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"\\nModel saved to {model_path}\")"
   ],
   "id": "e88c7bce866c6de",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
