{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d98f4e79af9ce6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:44:38.391934Z",
     "start_time": "2025-08-23T13:44:32.776817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in r:\\apps\\miniconda3\\lib\\site-packages (2.7.1+cu128)\n",
      "Requirement already satisfied: transformers in r:\\apps\\miniconda3\\lib\\site-packages (4.53.0)\n",
      "Requirement already satisfied: soundfile in r:\\apps\\miniconda3\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: moviepy in r:\\apps\\miniconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy in r:\\apps\\miniconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: pandas in r:\\apps\\miniconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in r:\\apps\\miniconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: faiss-cpu in r:\\apps\\miniconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in r:\\apps\\miniconda3\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in r:\\apps\\miniconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in r:\\apps\\miniconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in r:\\apps\\miniconda3\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in r:\\apps\\miniconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in r:\\apps\\miniconda3\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in r:\\apps\\miniconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in r:\\apps\\miniconda3\\lib\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: packaging>=20.0 in r:\\apps\\miniconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in r:\\apps\\miniconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in r:\\apps\\miniconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in r:\\apps\\miniconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in r:\\apps\\miniconda3\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in r:\\apps\\miniconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in r:\\apps\\miniconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: cffi>=1.0 in r:\\apps\\miniconda3\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in r:\\apps\\miniconda3\\lib\\site-packages (from moviepy) (5.1.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in r:\\apps\\miniconda3\\lib\\site-packages (from moviepy) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in r:\\apps\\miniconda3\\lib\\site-packages (from moviepy) (0.6.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in r:\\apps\\miniconda3\\lib\\site-packages (from moviepy) (0.1.12)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in r:\\apps\\miniconda3\\lib\\site-packages (from moviepy) (1.1.1)\n",
      "Requirement already satisfied: pillow<12.0,>=9.2.0 in r:\\apps\\miniconda3\\lib\\site-packages (from moviepy) (11.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in r:\\apps\\miniconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in r:\\apps\\miniconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in r:\\apps\\miniconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: click in r:\\apps\\miniconda3\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in r:\\apps\\miniconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: pycparser in r:\\apps\\miniconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in r:\\apps\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in r:\\apps\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in r:\\apps\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in r:\\apps\\miniconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in r:\\apps\\miniconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in r:\\apps\\miniconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in r:\\apps\\miniconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in r:\\apps\\miniconda3\\lib\\site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !pip install torch transformers soundfile moviepy numpy pandas nltk faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "834f6e3d243ddd2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:44:42.290003Z",
     "start_time": "2025-08-23T13:44:42.284750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HUGGINGFACE_HUB_CACHE=models\n"
     ]
    }
   ],
   "source": [
    "%env HUGGINGFACE_HUB_CACHE= models"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-08T13:31:24.030568Z",
     "start_time": "2025-09-08T13:31:23.046035Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from moviepy import VideoFileClip, AudioFileClip\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline, WhisperProcessor, \\\n",
    "    WhisperForConditionalGeneration\n",
    "# from nemo.collections.asr.models import ClusteringDiarizer\n",
    "import soundfile as sf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'moviepy'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmoviepy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m VideoFileClip, AudioFileClip\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnltk\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'moviepy'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "71e70a943c336cb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:44:45.193387Z",
     "start_time": "2025-08-23T13:44:45.188842Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c54699f2de8a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:44:45.993358Z",
     "start_time": "2025-08-23T13:44:45.985477Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_audio(input_file, output_folder=\"extracted_audio\"):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    base_name = os.path.basename(input_file)\n",
    "    file_name, file_ext = os.path.splitext(base_name)\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, f\"{file_name}.mp3\")\n",
    "\n",
    "    if file_ext.lower() == \".mp4\":\n",
    "        print(f\"Detected MP4 file. Extracting audio from '{input_file}'...\")\n",
    "        try:\n",
    "            video_clip = VideoFileClip(input_file)\n",
    "            audio_clip = video_clip.audio\n",
    "            audio_clip.write_audiofile(output_file_path)\n",
    "            audio_clip.close()\n",
    "            video_clip.close()\n",
    "            print(f\"Audio extracted successfully and saved to '{output_file_path}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during MP4 processing: {e}\")\n",
    "\n",
    "    elif file_ext.lower() == \".mp3\":\n",
    "        print(f\"Detected MP3 file. Copying '{input_file}'...\")\n",
    "        try:\n",
    "            with open(input_file, 'rb') as f_in, open(output_file_path, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "            return f_out\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during MP3 processing: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_ext}. Please provide an MP4 or MP3 file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c311f99137b6b767",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:44:52.079904Z",
     "start_time": "2025-08-23T13:44:46.628080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", torch_dtype=torch_dtype).to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "\n",
    "    generate_kwargs={\"language\": \"en\", \"task\": \"transcribe\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ab29c8c2bc82cd70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:44:58.476015Z",
     "start_time": "2025-08-23T13:44:58.469882Z"
    }
   },
   "outputs": [],
   "source": [
    "def audio_to_text(audio):\n",
    "    data, samplerate = sf.read(audio)\n",
    "\n",
    "    if len(data.shape) > 1:\n",
    "        mono_data = np.mean(data, axis=1)\n",
    "    else:\n",
    "        mono_data = data\n",
    "\n",
    "    audtext = pipe({\"array\": mono_data, \"sampling_rate\": samplerate}, return_timestamps=True)\n",
    "\n",
    "    return audtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "73453bf4c2f4aecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:44:59.195092Z",
     "start_time": "2025-08-23T13:44:58.971163Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6171eb9f626eed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:44:59.834542Z",
     "start_time": "2025-08-23T13:44:59.827567Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    words = text.lower().split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59498570680c1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:45:04.476867Z",
     "start_time": "2025-08-23T13:45:00.691208Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bbeae4ad2f4aedfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:45:04.936552Z",
     "start_time": "2025-08-23T13:45:04.931653Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    text_sentences = [s.strip() for s in re.split(r'[.?!]\\s+', text) if s.strip()]\n",
    "    text_embeddings = model.encode(text_sentences)\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c357f5f5e45152cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:47:19.983922Z",
     "start_time": "2025-08-23T13:47:19.976647Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimal_n_clusters(vectarray):\n",
    "    range_n_clusters = list(range(5, 30))\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for n_clusters in range_n_clusters:\n",
    "        clustering_model = KMeans(n_clusters=n_clusters, random_state=0, n_init=15)\n",
    "        cluster_labels = clustering_model.fit_predict(vectarray)\n",
    "\n",
    "        score = silhouette_score(vectarray, cluster_labels)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"Number of clusters: {n_clusters}, Silhouette Score: {score:.4f}\")\n",
    "\n",
    "    optimal_n_clusters = range_n_clusters[np.argmax(silhouette_scores)]\n",
    "\n",
    "    return optimal_n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1e0f1ab5aec43c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:47:21.131342Z",
     "start_time": "2025-08-23T13:47:21.125811Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_merger(vectarray, text):\n",
    "    n = optimal_n_clusters(vectarray)\n",
    "    final_clustering_model = KMeans(n_clusters=n, random_state=0, n_init=10)\n",
    "    merger_model = final_clustering_model.fit_predict(vectarray)\n",
    "    merged_sentences = []\n",
    "    merged_embeddings = []\n",
    "    for i in range(n):\n",
    "        cluster_indices = np.argwhere(merger_model == i).flatten()\n",
    "\n",
    "        if len(cluster_indices) > 0:\n",
    "            cluster_sentences = [text[j] for j in cluster_indices]\n",
    "            cluster_embeddings = vectarray[cluster_indices]\n",
    "\n",
    "            cluster_centroid = np.mean(cluster_embeddings, axis=0)\n",
    "            distances = np.linalg.norm(cluster_embeddings - cluster_centroid, axis=1)\n",
    "            closest_sentence_idx = np.argmin(distances)\n",
    "\n",
    "            representative_sentence = cluster_sentences[closest_sentence_idx]\n",
    "            merged_sentences.append(representative_sentence)\n",
    "            merged_embeddings.append(cluster_centroid)\n",
    "\n",
    "    return merged_embeddings, merged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c10e2732e4f3c252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:51:48.929601Z",
     "start_time": "2025-08-23T13:51:48.923658Z"
    }
   },
   "outputs": [],
   "source": [
    "def faiss_scoring(base_vectarray, test_vectarray):\n",
    "    test_vectarray = np.array(test_vectarray)\n",
    "    base_vectarray = np.array(base_vectarray)\n",
    "\n",
    "    faiss_index = faiss.IndexFlatIP(test_vectarray.shape[1])\n",
    "    faiss_index.add(test_vectarray)\n",
    "\n",
    "    distances, _ = faiss_index.search(base_vectarray, k=1)\n",
    "\n",
    "    similarity_scores = distances.flatten()\n",
    "\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da1594a96876f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_audio = extract_audio(r\"C:\\Users\\dudec\\OneDrive\\Studies\\Coursera\\Google_AI_Essentials\\AI and future of work.mp4\")\n",
    "\n",
    "test_audio = extract_audio(r\"C:\\Users\\dudec\\OneDrive\\Studies\\Coursera\\Google_AI_Essentials\\AI and future of work.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e4edd538f2820d96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:53:43.804621Z",
     "start_time": "2025-08-23T13:53:43.797940Z"
    }
   },
   "outputs": [],
   "source": [
    "base_text = \"\"\"To teach B.Tech students about word embeddings, start with a conceptual overview before diving into the technical details. Begin by explaining the fundamental problem: computers don't understand words as humans do. They need a numerical representation of text to perform any task. Traditional methods like one-hot encoding are simple but flawed. Illustrate one-hot encoding by showing that each word gets a unique vector of zeros with a single '1'. For example, if a vocabulary has 10,000 words, each word is a vector of 10,000 dimensions, which is extremely sparse and computationally inefficient.\n",
    "\n",
    "Highlight the biggest limitation of one-hot encoding: it treats every word as a completely independent entity. The words \"king\" and \"queen\" are as different to the computer as \"king\" and \"apple\". There is no way to capture the semantic relationships between words. This is where word embeddings come in. Define a word embedding as a way to represent words as dense, real-valued vectors in a lower-dimensional space. These vectors are designed to capture the meaning and relationships of words. Use an analogy: imagine a map where the location of a city corresponds to its vector. Cities that are geographically close, like Mumbai and Pune, are also close on the map. In the same way, words with similar meanings, like \"king\" and \"queen\", will have vectors that are numerically \"close\" in the embedding space.\n",
    "\n",
    "Introduce the core idea behind learning these embeddings: the distributional hypothesis. Simply put, words that appear in similar contexts have similar meanings. Use an example: \"The cat sat on the mat\" and \"The dog sat on the rug.\" The words \"cat\" and \"dog\" appear in similar contexts, suggesting they are related. This is the principle that many embedding models leverage.\n",
    "\n",
    "Next, introduce two of the most popular models: Word2Vec and GloVe. Explain Word2Vec as a predictive, neural network-based model. It has two main architectures: Continuous Bag of Words (CBOW) and Skip-Gram. Explain CBOW as predicting the current word from its surrounding context. For instance, given \"the cat sat on,\" the model predicts \"mat.\" Contrast this with Skip-Gram, which does the opposite: given a word like \"mat,\" it predicts the surrounding context words like \"the,\" \"cat,\" and \"on.\" Emphasize that training these models involves a shallow neural network and a large corpus of text. The final word vectors are the trained weights from the hidden layer.\n",
    "\n",
    "Then, discuss the GloVe (Global Vectors for Word Representation) model. Contrast it with Word2Vec by explaining that GloVe is a count-based model. It doesn't use a neural network to predict words. Instead, it leverages global co-occurrence statistics from the entire corpus. Explain that GloVe creates a co-occurrence matrix that counts how often words appear together, and then uses matrix factorization to generate the word vectors. The core idea is that ratios of co-occurrence probabilities can encode meaning.\n",
    "\n",
    "After explaining the models, show some practical applications. A classic example is vector arithmetic. Show that vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\"). This is a powerful demonstration of how embeddings capture complex semantic relationships. Another example is using cosine similarity to find the most similar words to a given word, like finding the closest words to \"computer\" would be \"laptop,\" \"PC,\" and \"server.\" Finally, touch upon the use of pre-trained embeddings, which are widely available and can save time and computational resources for many tasks.\"\"\"\n",
    "\n",
    "test_text = \"\"\"To teach B.Tech students about word embeddings, start with a conceptual overview before diving into the technical details. Begin by explaining the fundamental problem: computers don't understand words as humans do. They need a numerical representation of text to perform any task. Traditional methods like one-hot encoding are simple but flawed. Illustrate one-hot encoding by showing that each word gets a unique vector of zeros with a single '1'. For example, if a vocabulary has 10,000 words, each word is a vector of 10,000 dimensions, which is extremely sparse and computationally inefficient.\n",
    "\n",
    "Highlight the biggest limitation of one-hot encoding: it treats every word as a completely independent entity. The words \"king\" and \"queen\" are as different to the computer as \"king\" and \"apple\". There is no way to capture the semantic relationships between words. This is where word embeddings come in. Define a word embedding as a way to represent words as dense, real-valued vectors in a lower-dimensional space. These vectors are designed to capture the meaning and relationships of words. Use an analogy: imagine a map where the location of a city corresponds to its vector. Cities that are geographically close, like Mumbai and Pune, are also close on the map. In the same way, words with similar meanings, like \"king\" and \"queen\", will have vectors that are numerically \"close\" in the embedding space.\n",
    "\n",
    "Introduce the core idea behind learning these embeddings: the distributional hypothesis. Simply put, words that appear in similar contexts have similar meanings. Use an example: \"The cat sat on the mat\" and \"The dog sat on the rug.\" The words \"cat\" and \"dog\" appear in similar contexts, suggesting they are related. This is the principle that many embedding models leverage.\n",
    "\n",
    "After explaining the models, show some practical applications. A classic example is vector arithmetic. Show that vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\"). This is a powerful demonstration of how embeddings capture complex semantic relationships. Another example is using cosine similarity to find the most similar words to a given word, like finding the closest words to \"computer\" would be \"laptop,\" \"PC,\" and \"server.\" Finally, touch upon the use of pre-trained embeddings, which are widely available and can save time and computational resources for many tasks.\"\"\"\n",
    "\n",
    "# base_text = audio_to_text(base_audio)\n",
    "# test_text = audio_to_text(test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4057b5f79a3ecc97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:53:44.515527Z",
     "start_time": "2025-08-23T13:53:44.507452Z"
    }
   },
   "outputs": [],
   "source": [
    "base_text_lemmat = lemmatize_text(base_text)\n",
    "test_text_lemmat = lemmatize_text(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c38745f2545ea2e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:53:45.427383Z",
     "start_time": "2025-08-23T13:53:45.188770Z"
    }
   },
   "outputs": [],
   "source": [
    "base_vectors = text_to_vector(base_text)\n",
    "test_vectors = text_to_vector(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b89b79997a447e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:53:52.335700Z",
     "start_time": "2025-08-23T13:53:46.188169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 5, Silhouette Score: 0.0707\n",
      "Number of clusters: 6, Silhouette Score: 0.0753\n",
      "Number of clusters: 7, Silhouette Score: 0.0915\n",
      "Number of clusters: 8, Silhouette Score: 0.0937\n",
      "Number of clusters: 9, Silhouette Score: 0.0936\n",
      "Number of clusters: 10, Silhouette Score: 0.1017\n",
      "Number of clusters: 11, Silhouette Score: 0.0958\n",
      "Number of clusters: 12, Silhouette Score: 0.0928\n",
      "Number of clusters: 13, Silhouette Score: 0.0925\n",
      "Number of clusters: 14, Silhouette Score: 0.1063\n",
      "Number of clusters: 15, Silhouette Score: 0.0875\n",
      "Number of clusters: 16, Silhouette Score: 0.0938\n",
      "Number of clusters: 17, Silhouette Score: 0.0990\n",
      "Number of clusters: 18, Silhouette Score: 0.0888\n",
      "Number of clusters: 19, Silhouette Score: 0.0968\n",
      "Number of clusters: 20, Silhouette Score: 0.0934\n",
      "Number of clusters: 21, Silhouette Score: 0.0750\n",
      "Number of clusters: 22, Silhouette Score: 0.0866\n",
      "Number of clusters: 23, Silhouette Score: 0.0817\n",
      "Number of clusters: 24, Silhouette Score: 0.0715\n",
      "Number of clusters: 25, Silhouette Score: 0.0750\n",
      "Number of clusters: 26, Silhouette Score: 0.0602\n",
      "Number of clusters: 27, Silhouette Score: 0.0564\n",
      "Number of clusters: 28, Silhouette Score: 0.0469\n",
      "Number of clusters: 29, Silhouette Score: 0.0428\n"
     ]
    }
   ],
   "source": [
    "merged_base_vectors, merged_base_sentences = sentence_merger(base_vectors, base_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c828f9458c57409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:53:53.865094Z",
     "start_time": "2025-08-23T13:53:53.860768Z"
    }
   },
   "outputs": [],
   "source": [
    "faiss_symscore = faiss_scoring(merged_base_vectors, test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fab536ebc8c401ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:53:55.159611Z",
     "start_time": "2025-08-23T13:53:55.153856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37516057, 0.66274476, 0.66860807, 0.41999638, 0.46056998,\n",
       "       1.        , 0.7807846 , 0.64560837, 1.        , 0.99999994,\n",
       "       0.9999998 , 1.        , 1.        , 0.99999994], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_symscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cebf71fe3aada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Metrics \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'faiss_symscore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      6\u001B[39m teacher_duration_sec = \u001B[32m120\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mFinal Metrics \u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m similarity_scores = \u001B[43mfaiss_symscore\u001B[49m\n\u001B[32m     11\u001B[39m avg_semantic_score = np.mean(similarity_scores)\n\u001B[32m     12\u001B[39m percent_strong_matches = (\u001B[38;5;28mlen\u001B[39m([s \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m similarity_scores \u001B[38;5;28;01mif\u001B[39;00m s > \u001B[32m0.7\u001B[39m]) / \u001B[38;5;28mlen\u001B[39m(similarity_scores)) * \u001B[32m100\u001B[39m\n",
      "\u001B[31mNameError\u001B[39m: name 'faiss_symscore' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import textstat\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "base_duration_sec = 180\n",
    "teacher_duration_sec = 120\n",
    "\n",
    "\n",
    "similarity_scores = faiss_symscore\n",
    "avg_semantic_score = np.mean(similarity_scores)\n",
    "percent_strong_matches = (len([s for s in similarity_scores if s > 0.7]) / len(similarity_scores)) * 100\n",
    "\n",
    "teacher_vectors_normalized = teacher_vectors / np.linalg.norm(teacher_vectors, axis=1, keepdims=True)\n",
    "merged_base_vectors_normalized = merged_base_vectors / np.linalg.norm(merged_base_vectors, axis=1, keepdims=True)\n",
    "\n",
    "topic_index = faiss.IndexFlatIP(merged_base_vectors_normalized.shape[1])\n",
    "topic_index.add(merged_base_vectors_normalized)\n",
    "_, topic_assignments = topic_index.search(teacher_vectors_normalized, k=1)\n",
    "\n",
    "num_topics_covered = len(np.unique(topic_assignments))\n",
    "total_num_topics = len(merged_base_vectors)\n",
    "thematic_coverage_score = (num_topics_covered / total_num_topics) * 100\n",
    "\n",
    "base_word_count = len(re.findall(r'\\b[a-z]+\\b', base_text.lower()))\n",
    "teacher_word_count = len(re.findall(r'\\b[a-z]+\\b', test_text.lower()))\n",
    "base_fk_grade = textstat.flesch_kincaid_grade(base_text)\n",
    "teacher_fk_grade = textstat.flesch_kincaid_grade(test_text)\n",
    "base_wpm = (base_word_count / base_duration_sec) * 60 if base_duration_sec > 0 else 0\n",
    "teacher_wpm = (teacher_word_count / teacher_duration_sec) * 60 if teacher_duration_sec > 0 else 0\n",
    "\n",
    "report_data = {\n",
    "    \"Feature\": [\"Semantic Score (Avg)\", \"% Sentences > 70% Match\", \"Thematic Coverage\", \"Readability (F-K Grade)\", \"Pace (WPM)\"],\n",
    "    \"Base Lecture\": [\"---\", \"---\", \"---\", f\"{base_fk_grade:.2f}\", f\"{base_wpm:.2f}\"],\n",
    "    \"Teacher Lecture\": [\"---\", \"---\", \"---\", f\"{teacher_fk_grade:.2f}\", f\"{teacher_wpm:.2f}\"],\n",
    "    \"Comparison Score\": [f\"{avg_semantic_score:.4f}\", f\"{percent_strong_matches:.2f}%\", f\"{thematic_coverage_score:.2f}%\", \"---\", \"---\"]\n",
    "}\n",
    "report_df = pd.DataFrame(report_data)\n",
    "print(\"Report assembled.\")\n",
    "\n",
    "weights = {\n",
    "    \"Semantic\": 0.40, \"Coverage\": 0.25, \"Thematic\": 0.15,\n",
    "    \"Clarity\": 0.10, \"Pacing\": 0.10\n",
    "}\n",
    "\n",
    "clarity_score = min(base_fk_grade / teacher_fk_grade, 1.0) if teacher_fk_grade > 0 else 0\n",
    "pacing_score = min(teacher_wpm / base_wpm, 1.0) if base_wpm > 0 else 0\n",
    "\n",
    "final_score = (avg_semantic_score * weights[\"Semantic\"] + \n",
    "               (percent_strong_matches / 100) * weights[\"Coverage\"] +\n",
    "               (thematic_coverage_score / 100) * weights[\"Thematic\"] +\n",
    "               clarity_score * weights[\"Clarity\"] +\n",
    "               pacing_score * weights[\"Pacing\"])\n",
    "\n",
    "final_score_out_of_10 = final_score * 10\n",
    "print(\"Final score calculated.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=\"*50)\n",
    "print(report_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" Final Teacher Rating \")\n",
    "print(f\"The final calculated score for the teacher is: {final_score_out_of_10:.2f} / 10\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09044c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textstat in d:\\python\\research\\.venv\\lib\\site-packages (0.7.8)\n",
      "Requirement already satisfied: pyphen in d:\\python\\research\\.venv\\lib\\site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in d:\\python\\research\\.venv\\lib\\site-packages (from textstat) (1.1.1)\n",
      "Requirement already satisfied: setuptools in d:\\python\\research\\.venv\\lib\\site-packages (from textstat) (80.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in d:\\python\\research\\.venv\\lib\\site-packages (from cmudict->textstat) (8.7.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in d:\\python\\research\\.venv\\lib\\site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\python\\research\\.venv\\lib\\site-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86524a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
