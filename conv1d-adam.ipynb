{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-12T11:07:10.309651Z",
     "start_time": "2025-07-12T11:07:10.303967Z"
    }
   },
   "source": [
    "#Importing Packages\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as pre\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:42:20.026117Z",
     "start_time": "2025-07-12T10:42:20.020831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Move to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "59b1997730f08f4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:42:21.978052Z",
     "start_time": "2025-07-12T10:42:20.815632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Importing data from UNSW_NB15\n",
    "train_df = pd.read_csv('data/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('data/UNSW_NB15_testing-set.csv')"
   ],
   "id": "3a8cdff9faac9e5c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:42:22.878772Z",
     "start_time": "2025-07-12T10:42:22.802115Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.info()",
   "id": "70b49f8f7d6cd472",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175341 entries, 0 to 175340\n",
      "Data columns (total 45 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   id                 175341 non-null  int64  \n",
      " 1   dur                175341 non-null  float64\n",
      " 2   proto              175341 non-null  object \n",
      " 3   service            175341 non-null  object \n",
      " 4   state              175341 non-null  object \n",
      " 5   spkts              175341 non-null  int64  \n",
      " 6   dpkts              175341 non-null  int64  \n",
      " 7   sbytes             175341 non-null  int64  \n",
      " 8   dbytes             175341 non-null  int64  \n",
      " 9   rate               175341 non-null  float64\n",
      " 10  sttl               175341 non-null  int64  \n",
      " 11  dttl               175341 non-null  int64  \n",
      " 12  sload              175341 non-null  float64\n",
      " 13  dload              175341 non-null  float64\n",
      " 14  sloss              175341 non-null  int64  \n",
      " 15  dloss              175341 non-null  int64  \n",
      " 16  sinpkt             175341 non-null  float64\n",
      " 17  dinpkt             175341 non-null  float64\n",
      " 18  sjit               175341 non-null  float64\n",
      " 19  djit               175341 non-null  float64\n",
      " 20  swin               175341 non-null  int64  \n",
      " 21  stcpb              175341 non-null  int64  \n",
      " 22  dtcpb              175341 non-null  int64  \n",
      " 23  dwin               175341 non-null  int64  \n",
      " 24  tcprtt             175341 non-null  float64\n",
      " 25  synack             175341 non-null  float64\n",
      " 26  ackdat             175341 non-null  float64\n",
      " 27  smean              175341 non-null  int64  \n",
      " 28  dmean              175341 non-null  int64  \n",
      " 29  trans_depth        175341 non-null  int64  \n",
      " 30  response_body_len  175341 non-null  int64  \n",
      " 31  ct_srv_src         175341 non-null  int64  \n",
      " 32  ct_state_ttl       175341 non-null  int64  \n",
      " 33  ct_dst_ltm         175341 non-null  int64  \n",
      " 34  ct_src_dport_ltm   175341 non-null  int64  \n",
      " 35  ct_dst_sport_ltm   175341 non-null  int64  \n",
      " 36  ct_dst_src_ltm     175341 non-null  int64  \n",
      " 37  is_ftp_login       175341 non-null  int64  \n",
      " 38  ct_ftp_cmd         175341 non-null  int64  \n",
      " 39  ct_flw_http_mthd   175341 non-null  int64  \n",
      " 40  ct_src_ltm         175341 non-null  int64  \n",
      " 41  ct_srv_dst         175341 non-null  int64  \n",
      " 42  is_sm_ips_ports    175341 non-null  int64  \n",
      " 43  attack_cat         175341 non-null  object \n",
      " 44  label              175341 non-null  int64  \n",
      "dtypes: float64(11), int64(30), object(4)\n",
      "memory usage: 60.2+ MB\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:42:23.825950Z",
     "start_time": "2025-07-12T10:42:23.787709Z"
    }
   },
   "cell_type": "code",
   "source": "test_df.info()",
   "id": "35c969b3ddc43ad7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82332 entries, 0 to 82331\n",
      "Data columns (total 45 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 82332 non-null  int64  \n",
      " 1   dur                82332 non-null  float64\n",
      " 2   proto              82332 non-null  object \n",
      " 3   service            82332 non-null  object \n",
      " 4   state              82332 non-null  object \n",
      " 5   spkts              82332 non-null  int64  \n",
      " 6   dpkts              82332 non-null  int64  \n",
      " 7   sbytes             82332 non-null  int64  \n",
      " 8   dbytes             82332 non-null  int64  \n",
      " 9   rate               82332 non-null  float64\n",
      " 10  sttl               82332 non-null  int64  \n",
      " 11  dttl               82332 non-null  int64  \n",
      " 12  sload              82332 non-null  float64\n",
      " 13  dload              82332 non-null  float64\n",
      " 14  sloss              82332 non-null  int64  \n",
      " 15  dloss              82332 non-null  int64  \n",
      " 16  sinpkt             82332 non-null  float64\n",
      " 17  dinpkt             82332 non-null  float64\n",
      " 18  sjit               82332 non-null  float64\n",
      " 19  djit               82332 non-null  float64\n",
      " 20  swin               82332 non-null  int64  \n",
      " 21  stcpb              82332 non-null  int64  \n",
      " 22  dtcpb              82332 non-null  int64  \n",
      " 23  dwin               82332 non-null  int64  \n",
      " 24  tcprtt             82332 non-null  float64\n",
      " 25  synack             82332 non-null  float64\n",
      " 26  ackdat             82332 non-null  float64\n",
      " 27  smean              82332 non-null  int64  \n",
      " 28  dmean              82332 non-null  int64  \n",
      " 29  trans_depth        82332 non-null  int64  \n",
      " 30  response_body_len  82332 non-null  int64  \n",
      " 31  ct_srv_src         82332 non-null  int64  \n",
      " 32  ct_state_ttl       82332 non-null  int64  \n",
      " 33  ct_dst_ltm         82332 non-null  int64  \n",
      " 34  ct_src_dport_ltm   82332 non-null  int64  \n",
      " 35  ct_dst_sport_ltm   82332 non-null  int64  \n",
      " 36  ct_dst_src_ltm     82332 non-null  int64  \n",
      " 37  is_ftp_login       82332 non-null  int64  \n",
      " 38  ct_ftp_cmd         82332 non-null  int64  \n",
      " 39  ct_flw_http_mthd   82332 non-null  int64  \n",
      " 40  ct_src_ltm         82332 non-null  int64  \n",
      " 41  ct_srv_dst         82332 non-null  int64  \n",
      " 42  is_sm_ips_ports    82332 non-null  int64  \n",
      " 43  attack_cat         82332 non-null  object \n",
      " 44  label              82332 non-null  int64  \n",
      "dtypes: float64(11), int64(30), object(4)\n",
      "memory usage: 28.3+ MB\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:27:52.651738Z",
     "start_time": "2025-07-12T10:27:52.637116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #Encoding Labels in Train Dataset\n",
    "#\n",
    "# # train_df['attack_cat'] = train_df['attack_cat'].map(\n",
    "# #     {\n",
    "# #   \"Analysis\": 0,\n",
    "# #   \"Backdoor\": 1,\n",
    "# #   \"DoS\": 2,\n",
    "# #   \"Exploits\": 3,\n",
    "# #   \"Fuzzers\": 4,\n",
    "# #   \"Generic\": 5,\n",
    "# #   \"Normal\": 6,\n",
    "# #   \"Reconnaissance\": 7,\n",
    "# #   \"Shellcode\": 8,\n",
    "# #   \"Worms\": 9\n",
    "# # }\n",
    "# # )\n",
    "#\n",
    "# train_df = pd.DataFrame({'attack_cat': [\"Analysis\", \"Backdoor\", \"Normal\", \"Exploits\", \"Shellcode\", \"Fuzzers\", \"Generic\", \"Reconnaissance\", \"DoS\", \"Worms\"]})\n",
    "#\n",
    "# le = pre.LabelEncoder()\n",
    "# train_df['attack_cat_encoded'] = le.fit_transform(train_df['attack_cat'])\n"
   ],
   "id": "12791622d6c4954b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:27:53.574328Z",
     "start_time": "2025-07-12T10:27:53.564896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #Encoding Labels in Test Dataset\n",
    "# test_df = pd.DataFrame({'attack_cat': [\"Analysis\", \"Backdoor\", \"Normal\", \"Exploits\", \"Shellcode\", \"Fuzzers\", \"Generic\", \"Reconnaissance\", \"DoS\", \"Worms\"]})\n",
    "#\n",
    "# le = pre.LabelEncoder()\n",
    "# test_df['attack_cat_encoded'] = le.fit_transform(test_df['attack_cat'])"
   ],
   "id": "3d583be096027865",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T11:07:21.657702Z",
     "start_time": "2025-07-12T11:07:21.629421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Collecting required features\n",
    "feature_columns = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sload', 'dload', 'smean', 'dmean',]# 'attack_cat_encoded'\n",
    "\n",
    "X_train = train_df[feature_columns].values\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "\n",
    "le = pre.LabelEncoder()\n",
    "train_df['attack_cat_encoded'] = le.fit_transform(train_df['attack_cat'])\n",
    "\n",
    "X_test = test_df[feature_columns].values\n",
    "y_test = test_df['label'].values"
   ],
   "id": "d879d96c98190a1f",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T11:07:23.115459Z",
     "start_time": "2025-07-12T11:07:23.046206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Converting to tensor\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converting to tensor and moving to the selected device\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)"
   ],
   "id": "81120687472a7ab",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T11:07:24.190110Z",
     "start_time": "2025-07-12T11:07:24.179271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Data loaded from CSV and prepared for PyTorch.\")\n",
    "print(\"Training data shape:\", X_train_tensor.shape)\n",
    "print(\"Test data shape:\", X_test_tensor.shape)\n",
    "print(f\"Number of unique labels (classes): {len(np.unique(y_train))}\")\n"
   ],
   "id": "6f4f459bdb0bd14b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from CSV and prepared for PyTorch.\n",
      "Training data shape: torch.Size([175341, 1, 10])\n",
      "Test data shape: torch.Size([82332, 1, 10])\n",
      "Number of unique labels (classes): 2\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T11:07:25.270962Z",
     "start_time": "2025-07-12T11:07:25.260479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net1DCNN(nn.Module):\n",
    "    def __init__(self, input_channels, sequence_length, num_classes):\n",
    "        super(Net1DCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # self.fc1 = nn.Linear(160 * 1, 64)\n",
    "        # self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, input_channels, sequence_length)\n",
    "            dummy_output = self.pool1(self.relu1(self.conv1(dummy_input)))\n",
    "            dummy_output = self.pool2(self.relu2(self.conv2(dummy_output)))\n",
    "            flattened_size = dummy_output.view(dummy_output.size(0), -1).shape[1]\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_size, 128)\n",
    "        self.relu_fc = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "id": "6170ce8e879f03f9",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T12:49:56.088920Z",
     "start_time": "2025-07-12T12:34:36.364275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_channels = X_train_tensor.shape[1]\n",
    "sequence_length = X_train_tensor.shape[2]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "model = Net1DCNN(input_channels=input_channels, sequence_length=sequence_length, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ],
   "id": "33716de3afbcb857",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Epoch [1/50], Loss: 0.2677\n",
      "Epoch [2/50], Loss: 0.2069\n",
      "Epoch [3/50], Loss: 0.1879\n",
      "Epoch [4/50], Loss: 0.1758\n",
      "Epoch [5/50], Loss: 0.1668\n",
      "Epoch [6/50], Loss: 0.1615\n",
      "Epoch [7/50], Loss: 0.1585\n",
      "Epoch [8/50], Loss: 0.1560\n",
      "Epoch [9/50], Loss: 0.1543\n",
      "Epoch [10/50], Loss: 0.1520\n",
      "Epoch [11/50], Loss: 0.1508\n",
      "Epoch [12/50], Loss: 0.1502\n",
      "Epoch [13/50], Loss: 0.1488\n",
      "Epoch [14/50], Loss: 0.1472\n",
      "Epoch [15/50], Loss: 0.1459\n",
      "Epoch [16/50], Loss: 0.1452\n",
      "Epoch [17/50], Loss: 0.1443\n",
      "Epoch [18/50], Loss: 0.1436\n",
      "Epoch [19/50], Loss: 0.1430\n",
      "Epoch [20/50], Loss: 0.1425\n",
      "Epoch [21/50], Loss: 0.1413\n",
      "Epoch [22/50], Loss: 0.1411\n",
      "Epoch [23/50], Loss: 0.1405\n",
      "Epoch [24/50], Loss: 0.1403\n",
      "Epoch [25/50], Loss: 0.1394\n",
      "Epoch [26/50], Loss: 0.1391\n",
      "Epoch [27/50], Loss: 0.1382\n",
      "Epoch [28/50], Loss: 0.1377\n",
      "Epoch [29/50], Loss: 0.1375\n",
      "Epoch [30/50], Loss: 0.1370\n",
      "Epoch [31/50], Loss: 0.1364\n",
      "Epoch [32/50], Loss: 0.1359\n",
      "Epoch [33/50], Loss: 0.1354\n",
      "Epoch [34/50], Loss: 0.1353\n",
      "Epoch [35/50], Loss: 0.1346\n",
      "Epoch [36/50], Loss: 0.1348\n",
      "Epoch [37/50], Loss: 0.1339\n",
      "Epoch [38/50], Loss: 0.1334\n",
      "Epoch [39/50], Loss: 0.1330\n",
      "Epoch [40/50], Loss: 0.1327\n",
      "Epoch [41/50], Loss: 0.1326\n",
      "Epoch [42/50], Loss: 0.1324\n",
      "Epoch [43/50], Loss: 0.1316\n",
      "Epoch [44/50], Loss: 0.1336\n",
      "Epoch [45/50], Loss: 0.1352\n",
      "Epoch [46/50], Loss: 0.1323\n",
      "Epoch [47/50], Loss: 0.1317\n",
      "Epoch [48/50], Loss: 0.1340\n",
      "Epoch [49/50], Loss: 0.1336\n",
      "Epoch [50/50], Loss: 0.1301\n",
      "Training finished.\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T12:55:11.516587Z",
     "start_time": "2025-07-12T12:55:07.569172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move inputs and labels to the device for each batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"\\nAccuracy on the test set: {accuracy:.2f}%\")"
   ],
   "id": "40a63e320bb1cf8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on the test set: 84.30%\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_dir = 'models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, '1d_cnn_model.pth')\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"\\nModel saved to {model_path}\")"
   ],
   "id": "e88c7bce866c6de",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
